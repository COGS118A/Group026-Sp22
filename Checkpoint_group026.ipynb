{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 118A- Project Checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Names\n",
    "\n",
    "Hopefully your team is at least this good. Obviously you should replace these with your names.\n",
    "\n",
    "- Dylan Nelson\n",
    "- Mengyu Zhang\n",
    "- Shengjie Mao\n",
    "- Yufei Deng"
    


   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "This project is intended to design a machine learning algorithm that takes images of dogs and outputs their breed names accordingly. The dataset we will be using is adapted from the Stanford Dogs Dataset, which has a total of 20,580 individual dog images of 120 different breed categories. We plan on using Convolutional Neural Networks (CNN) for the image classification task. Before feeding input images into the neural nets, we will explore image transformation methods (e.g., image cropping/ resizing, converting to greyscale) to ensure the maximal performance of the CNN with the least amount of information required. The performance of models will be measured using the two metrics for classification: Accuracy and Area Under Curve (AUC). Lastly, we will apply data visualization tools such as TSNE to the classification results, which helps visualize the separability of the clusters of dog breeds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "Hunting continues to make many animal species extinct, and the governments are unable to do much except introducing some laws and conducting regular surveys. But conducting a survey is not an easy task, especially without the help of technology.[1]\n",
    "\n",
    "Camera trap could help us with this, camera Trap is a camera that is automatically triggered bya change in some activity in its vicinity , like presence of an animal. [2]\n",
    "\n",
    "\n",
    "In order for the camera trap to match the image with the animal in the wild, we would have to train the models with the animals pictures. We could start with the animal that we are most familiar with: dogs. With the results we get from analyzing the data of dogs, we might be able to find the most efficient way to categorize the images of other animals. \n",
    "Camera trap imaging (automatic photography of animal species in the wild) is becoming the gold standard in biodiversity conservation efforts. It allows for accurately monitoring large swaths of land at an unprecedented scale.[3] \n",
    "\n",
    "Instead of sending people to the wild and do massive work, we can just train our model with existing pictures of the animals we want to capture and put cameras out there, we will be able to know the time, location and what kind of animals passed through. Also, this method could also help us in other way, “Cameras Designed To Find Rare Animals Could Catch Poachers Instead”.[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "We want to create a model that can detect different breeds of dogs from images. This could be readily applied in many ways: field research, live cameras, search engine classification, etc. From our background research, we saw not many models had promising results. We’d like to explore this ourselves, look at the problems we face along the way, and either improve them, or at strengthen our understanding of the limitations of ML methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "Datasets: Stanford Dogs Dataset: http://vision.stanford.edu/aditya86/ImageNetDogs/\n",
    "\n",
    "You should have obtained and cleaned (if necessary) data you will use for this project.\n",
    "\n",
    "Please give the following infomration for each dataset you are using\n",
    "- link/reference to obtain it\n",
    "- description of the size of the dataset (# of variables, # of observations)\n",
    "- what an observation consists of\n",
    "- what some critical variables are, how they are represented\n",
    "- any special handling, transformations, cleaning, etc you have done should be demonstrated here!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is 120 breeds of dogs with pictures of each, as well as annotations for each image if we chose to use those. With 20k pictures we believe we can use the pixel data from each image to develop features to do a classification problem\n",
    "\n",
    "**Dataset Stats**  \n",
    "Total Pics: 20,580  \n",
    "Total Species: 120\n",
    "\n",
    "Average # pics: \t 171.5  \n",
    "Minimum # pics: \t 148  \n",
    "Maximum # pics: \t 252  \n",
    "\n",
    "**Image Stats**  \n",
    "Mean img Height: \t 385.861  \n",
    "Mean img Width: \t 442.532  \n",
    "\n",
    "Med img Height: \t 375.0  \n",
    "Med img Width: \t\t 500.0  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![output.png](output.png)\n",
    "*There are no low outliers (too little info), only breeds with a relatively large amount of pics*\n",
    "![output2.png](output2.png)\n",
    "*even the lowest 50 breeds have a range of 8 images, little to no underrepresentation*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "\n",
    "We plan to use a deep learning and neural network approach to solve the image classification task. We will implement the Conolutional Neural Network method from python libraries such as Tensorflow, keras, etc. We will also be testing out dimensionality reduction algorithms such as PCA as part of the pre-processing steps. The PCA will allow us to single out the most important features of our images before feeding them into the neural networks, which could help boost model performance.\n",
    "\n",
    "For training, testing, and model selection, we will be importing relevent packages from the Scikit Learn library. We plan on doing a 80/20 training-and-testing-set split, then applying k-fold-cross-validation to select the model with the best training accuracy.\n",
    "\n",
    "We also found another group who used Sci Kit learn to almost entirely do a similar problem. They divided their data, selected a model, and processed their data using this package. In their methods they only used ~5 different categories. Our problem requires many more classes but we can see how well this solution scales. Their processing basically took images and converted it into a grid of vectors that helped the model detect shapes and features better. Following that they used stochastic gradient descent as their model choice and had promising results we’d like to test out ourselves.\n",
    "\n",
    "Ultimately, we found a large variety of possible methods we could take and would like to try a few. If they don’t work and given we still have time, we could also look for even more methods elsewhere. Or possibly even combine methods in something like a decision tree learning style.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "Different Approaches at Accuracy\n",
    "- comparing different sample distributions: It is important we chose a sample to test our data on that is fair. Two ways we can do that is equal representation of each class, or representation of each class that represents their real world distributions. Since we aren’t providing this as a service for a specific audience, these are the most generalizable options. Since the real world distribution changes and we don’t have a preference towards one breed vs others, it may be most fair to have each breed be equally represented, assuming the data is strong enough to support that\n",
    "- Calculations: Since there is no discrete data, and the outputs are purely categorical, calculating distances or most other math-based metrics are not necessary. Some of the following metrics will be tracked and accounted for across models:\n",
    "\n",
    "(Precision, Recall, Accuracy, F1socre)\n",
    "\n",
    "ROC-AUC\n",
    "Since our data has more than 2 classes, we will use a “One vs. Rest” scheme for drawing the ROC curve and calculating the AUC score, specifically by reducing all classes other than the target class to one “negative” class. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Data collection: We retrieved the dataset from Stanford dataset, which means that we did not have direct access to the participants. Since the library is a public platform, we believe that we are allowed to use the data for our analysis and will not hold bias toward the contents of the data;\n",
    "- Data storage: We can assume that the data is safe and will not be exposed to others for illegal use since we only process and analyze the data using Google drive and Github associated with our UCSD email addresses. We will also delete the data from our local computer once we finish the project;\n",
    "- Analysis: We will use visualizations along with the codes and descriptions in our analysis so that people can understand our analysis clearly. We will make well documented analysis to make it reproducible later;\n",
    "- Modeling: We will not rely on discriminated data. And since we evaluate the dog datasets, we will not have ethical concerns about race, genders, etc;\n",
    "- Deployment: We will not allow unintended use of the model by tracking the activities on our Github homepage. We believe that the model can help us have better understanding toward the breeds of the dog.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethics & Privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your project has obvious potential concerns with ethics or data privacy discuss that here.  Almost every ML project put into production can have ethical implications if you use your imagination. Use your imagination.\n",
    "\n",
    "Even if you can't come up with an obvious ethical concern that should be addressed, you should know that a large number of ML projects that go into producation have unintended consequences and ethical problems once in production. How will your team address these issues?\n",
    "\n",
    "Consider a tool to help you address the potential issues such as https://deon.drivendata.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team Expectations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put things here that cement how you will interact/communicate as a team, how you will handle conflict and difficulty, how you will handle making decisions and setting goals/schedule, how much work you expect from each other, how you will handle deadlines, etc...\n",
    "* *Team Expectation 1*\n",
    "* *Team Expectation 2*\n",
    "* *Team Expecation 3*\n",
    "* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Timeline Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UPDATE THE PROPOSAL TIMELINE ACCORDING TO WHAT HAS ACTUALLY HAPPENED AND HOW IT HAS EFFECTED YOUR FUTURE PLANS\n",
    "\n",
    "| Meeting Date  | Meeting Time| Completed Before Meeting  | Discuss at Meeting |\n",
    "|---|---|---|---|\n",
    "| 1/20  |  1 PM |  Brainstorm topics/questions (all)  | Determine best form of communication; Discuss and decide on final project topic; discuss hypothesis; begin background research | \n",
    "| 1/26  |  10 AM |  Do background research on topic (Pelé) | Discuss ideal dataset(s) and ethics; draft project proposal | \n",
    "| 2/1  | 10 AM  | Edit, finalize, and submit proposal; Search for datasets (Beckenbaur)  | Discuss Wrangling and possible analytical approaches; Assign group members to lead each specific part   |\n",
    "| 2/14  | 6 PM  | Import & Wrangle Data ,do some EDA (Maradonna) | Review/Edit wrangling/EDA; Discuss Analysis Plan   |\n",
    "| 2/23  | 12 PM  | Finalize wrangling/EDA; Begin programming for project (Cruyff) | Discuss/edit project code; Complete project |\n",
    "| 3/13  | 12 PM  | Complete analysis; Draft results/conclusion/discussion (Carlos)| Discuss/edit full project |\n",
    "| 3/19  | Before 11:59 PM  | NA | Turn in Final Project  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "<a name=\"lorenznote\"></a>1.[^](#lorenz): Lorenz, T. (9 Dec 2021) Birds Aren’t Real, or Are They? Inside a Gen Z Conspiracy Theory. *The New York Times*. https://www.nytimes.com/2021/12/09/technology/birds-arent-real-gen-z-misinformation.html<br> \n",
    "<a name=\"admonishnote\"></a>2.[^](#admonish): Also refs should be important to the background, not some randomly chosen vaguely related stuff. Include a web link if possible in refs as above.<br>\n",
    "<a name=\"sotanote\"></a>3.[^](#sota): Perhaps the current state of the art solution such as you see on [Papers with code](https://paperswithcode.com/sota). Or maybe not SOTA, but rather a standard textbook/Kaggle solution to this kind of problem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
